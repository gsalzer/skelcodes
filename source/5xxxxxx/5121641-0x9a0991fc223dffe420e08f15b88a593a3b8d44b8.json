{"status":"1","message":"OK","result":[{"SourceCode":"pragma solidity ^0.4.19;\r\n// Danku contract version 0.0.1\r\n// Data points are x, y, and z\r\n\r\ncontract Danku_demo {\r\n  function Danku_demo() public {\r\n    // Neural Network Structure:\r\n    //\r\n    // (assertd) input layer x number of neurons\r\n    // (optional) hidden layers x number of neurons\r\n    // (assertd) output layer x number of neurons\r\n  }\r\n  struct Submission {\r\n      address payment_address;\r\n      // Define the number of neurons each layer has.\r\n      uint num_neurons_input_layer;\r\n      uint num_neurons_output_layer;\r\n      // There can be multiple hidden layers.\r\n      uint[] num_neurons_hidden_layer;\r\n      // Weights indexes are the following:\r\n      // weights[l_i x l_n_i x pl_n_i]\r\n      // Also number of layers in weights is layers.length-1\r\n      int256[] weights;\r\n      int256[] biases;\r\n  }\r\n  struct NeuralLayer {\r\n    int256[] neurons;\r\n    int256[] errors;\r\n    string layer_type;\r\n  }\r\n\r\n  address public organizer;\r\n  // Keep track of the best model\r\n  uint public best_submission_index;\r\n  // Keep track of best model accuracy\r\n  int256 public best_submission_accuracy = 0;\r\n  // The model accuracy criteria\r\n  int256 public model_accuracy_criteria;\r\n  // Use test data if provided\r\n  bool public use_test_data = false;\r\n  // Each partition is 5% of the total dataset size\r\n  uint constant partition_size = 25;\r\n  // Data points are made up of x and y coordinates and the prediction\r\n  uint constant datapoint_size = 3;\r\n  uint constant prediction_size = 1;\r\n  // Max number of data groups\r\n  // Change this to your data group size\r\n  uint16 constant max_num_data_groups = 500;\r\n  // Training partition size\r\n  uint16 constant training_data_group_size = 400;\r\n  // Testing partition size\r\n  uint16 constant testing_data_group_size = max_num_data_groups - training_data_group_size;\r\n  // Dataset is divided into data groups.\r\n  // Every data group includes a nonce.\r\n  // Look at sha_data_group() for more detail about hashing a data group\r\n  bytes32[max_num_data_groups/partition_size] hashed_data_groups;\r\n  // Nonces are revelead together with data groups\r\n  uint[max_num_data_groups/partition_size] data_group_nonces;\r\n  // + 1 for prediction\r\n  // A data group has 3 data points in total\r\n  int256[datapoint_size][] public train_data;\r\n  int256[datapoint_size][] public test_data;\r\n  bytes32 partition_seed;\r\n  // Deadline for submitting solutions in terms of block size\r\n  uint public submission_stage_block_size = 241920; // 6 weeks timeframe\r\n  // Deadline for revealing the testing dataset\r\n  uint public reveal_test_data_groups_block_size = 17280; // 3 days timeframe\r\n  // Deadline for evaluating the submissions\r\n  uint public evaluation_stage_block_size = 40320; // 7 days timeframe\r\n  uint public init1_block_height;\r\n  uint public init3_block_height;\r\n  uint public init_level = 0;\r\n  // Training partition size is 14 (70%)\r\n  // Testing partition size is 6 (30%)\r\n  uint[training_data_group_size/partition_size] public training_partition;\r\n  uint[testing_data_group_size/partition_size] public testing_partition;\r\n  uint256 train_dg_revealed = 0;\r\n  uint256 test_dg_revealed = 0;\r\n  Submission[] submission_queue;\r\n  bool public contract_terminated = false;\r\n  // Integer precision for calculating float values for weights and biases\r\n  int constant int_precision = 10000;\r\n\r\n  // Takes in array of hashed data points of the entire dataset,\r\n  // submission and evaluation times\r\n  function init1(bytes32[max_num_data_groups/partition_size] _hashed_data_groups, int accuracy_criteria, address organizer_refund_address) external {\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Make sure it's called in order\r\n    assert(init_level == 0);\r\n    organizer = organizer_refund_address;\r\n    init_level = 1;\r\n    init1_block_height = block.number;\r\n\r\n    // Make sure there are in total 20 hashed data groups\r\n    assert(_hashed_data_groups.length == max_num_data_groups/partition_size);\r\n    hashed_data_groups = _hashed_data_groups;\r\n    // Accuracy criteria example: 85.9% => 8,590\r\n    // 100 % => 10,000\r\n    assert(accuracy_criteria > 0);\r\n    model_accuracy_criteria = accuracy_criteria;\r\n  }\r\n\r\n  function init2() external {\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Only allow calling it once, in order\r\n    assert(init_level == 1);\r\n    // Make sure it's being called within 20 blocks on init1()\r\n    // to minimize organizer influence on random index selection\r\n    if (block.number <= init1_block_height+20 && block.number > init1_block_height) {\r\n      // TODO: Also make sure it's being called 1 block after init1()\r\n      // Randomly select indexes\r\n      uint[] memory index_array = new uint[](max_num_data_groups/partition_size);\r\n      for (uint i = 0; i < max_num_data_groups/partition_size; i++) {\r\n        index_array[i] = i;\r\n      }\r\n      randomly_select_index(index_array);\r\n      init_level = 2;\r\n    } else {\r\n      // Cancel the contract if init2() hasn't been called within 5\r\n      // blocks of init1()\r\n      cancel_contract();\r\n    }\r\n  }\r\n\r\n  function init3(int256[] _train_data_groups, int256 _train_data_group_nonces) external {\r\n    // Pass a single data group at a time\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Only allow calling once, in order\r\n    assert(init_level == 2);\r\n    // Verify data group and nonce lengths\r\n    assert((_train_data_groups.length/partition_size)/datapoint_size == 1);\r\n    // Verify data group hashes\r\n    // Order of revealed training data group must be the same with training partitions\r\n    // Otherwise hash verification will fail\r\n    assert(sha_data_group(_train_data_groups, _train_data_group_nonces) ==\r\n      hashed_data_groups[training_partition[train_dg_revealed]]);\r\n    train_dg_revealed += 1;\r\n    // Assign training data after verifying the corresponding hash\r\n    unpack_data_groups(_train_data_groups, true);\r\n    if (train_dg_revealed == (training_data_group_size/partition_size)) {\r\n      init_level = 3;\r\n      init3_block_height = block.number;\r\n    }\r\n  }\r\n\r\n  function get_training_index() public view returns(uint[training_data_group_size/partition_size]) {\r\n    return training_partition;\r\n  }\r\n\r\n  function get_testing_index() public view returns(uint[testing_data_group_size/partition_size]) {\r\n    return testing_partition;\r\n  }\r\n\r\n  function get_submission_queue_length() public view returns(uint) {\r\n    return submission_queue.length;\r\n  }\r\n\r\n  function submit_model(\r\n    // Public function for users to submit a solution\r\n    address payment_address,\r\n    uint num_neurons_input_layer,\r\n    uint num_neurons_output_layer,\r\n    uint[] num_neurons_hidden_layer,\r\n    int[] weights,\r\n    int256[] biases) public {\r\n      // Make sure contract is not terminated\r\n      assert(contract_terminated == false);\r\n      // Make sure it's not the initialization stage anymore\r\n      assert(init_level == 3);\r\n      // Make sure it's still within the submission stage\r\n      assert(block.number < init3_block_height + submission_stage_block_size);\r\n      // Make sure that num of neurons in the input & output layer matches\r\n      // the problem description\r\n      assert(num_neurons_input_layer == datapoint_size - prediction_size);\r\n      // Because we can encode binary output in two different ways, we check\r\n      // for both of them\r\n      assert(num_neurons_output_layer == prediction_size || num_neurons_output_layer == (prediction_size+1));\r\n      // Make sure that the number of weights match network structure\r\n      assert(valid_weights(weights, num_neurons_input_layer, num_neurons_output_layer, num_neurons_hidden_layer));\r\n      // Add solution to submission queue\r\n      submission_queue.push(Submission(\r\n        payment_address,\r\n        num_neurons_input_layer,\r\n        num_neurons_output_layer,\r\n        num_neurons_hidden_layer,\r\n        weights,\r\n        biases));\r\n  }\r\n\r\n  function get_submission_id(\r\n    // Public function that returns the submission index ID\r\n    address paymentAddress,\r\n    uint num_neurons_input_layer,\r\n    uint num_neurons_output_layer,\r\n    uint[] num_neurons_hidden_layer,\r\n    int[] weights,\r\n    int256[] biases) public view returns (uint) {\r\n      // Iterate over submission queue to get submission index ID\r\n      for (uint i = 0; i < submission_queue.length; i++) {\r\n        if (submission_queue[i].payment_address != paymentAddress) {\r\n          continue;\r\n        }\r\n        if (submission_queue[i].num_neurons_input_layer != num_neurons_input_layer) {\r\n          continue;\r\n        }\r\n        if (submission_queue[i].num_neurons_output_layer != num_neurons_output_layer) {\r\n          continue;\r\n        }\r\n        for (uint j = 0; j < num_neurons_hidden_layer.length; j++) {\r\n            if (submission_queue[i].num_neurons_hidden_layer[j] != num_neurons_hidden_layer[j]) {\r\n              continue;\r\n            }\r\n        }\r\n        for (uint k = 0; k < weights.length; k++) {\r\n            if (submission_queue[i].weights[k] != weights[k]) {\r\n              continue;\r\n            }\r\n        }\r\n        for (uint l = 0; l < biases.length; l++) {\r\n          if (submission_queue[i].biases[l] != biases[l]) {\r\n            continue;\r\n          }\r\n        }\r\n        // If everything matches, return the submission index\r\n        return i;\r\n      }\r\n      // If submission is not in the queue, just throw an exception\r\n      require(false);\r\n  }\r\n\r\n    function reveal_test_data(int256[] _test_data_groups, int256 _test_data_group_nonces) external {\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Make sure it's not the initialization stage anymore\r\n    assert(init_level == 3);\r\n    // Make sure it's revealed after the submission stage\r\n    assert(block.number >= init3_block_height + submission_stage_block_size);\r\n    // Make sure it's revealed within the reveal stage\r\n    assert(block.number < init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size);\r\n    // Verify data group and nonce lengths\r\n    assert((_test_data_groups.length/partition_size)/datapoint_size == 1);\r\n    // Verify data group hashes\r\n    assert(sha_data_group(_test_data_groups, _test_data_group_nonces) ==\r\n      hashed_data_groups[testing_partition[test_dg_revealed]]);\r\n    test_dg_revealed += 1;\r\n    // Assign testing data after verifying the corresponding hash\r\n    unpack_data_groups(_test_data_groups, false);\r\n    // Use test data for evaluation\r\n    use_test_data = true;\r\n  }\r\n\r\n  function evaluate_model(uint submission_index) public {\r\n    // TODO: Make sure that if there's two same submission w/ same weights\r\n    // and biases, the first one submitted should get the reward.\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Make sure it's not the initialization stage anymore\r\n    assert(init_level == 3);\r\n    // Make sure it's evaluated after the reveal stage\r\n    assert(block.number >= init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size);\r\n    // Make sure it's evaluated within the evaluation stage\r\n    assert(block.number < init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size + evaluation_stage_block_size);\r\n    // Evaluates a submitted model & keeps track of the best model\r\n    int256 submission_accuracy = 0;\r\n    if (use_test_data == true) {\r\n      submission_accuracy = model_accuracy(submission_index, test_data);\r\n    } else {\r\n      submission_accuracy = model_accuracy(submission_index, train_data);\r\n    }\r\n\r\n    // Keep track of the most accurate model\r\n    if (submission_accuracy > best_submission_accuracy) {\r\n      best_submission_index = submission_index;\r\n      best_submission_accuracy = submission_accuracy;\r\n    }\r\n  }\r\n\r\n  function cancel_contract() public {\r\n    // Make sure contract is not already terminated\r\n    assert(contract_terminated == false);\r\n    // Contract can only be cancelled if initialization has failed.\r\n    assert(init_level < 3);\r\n    // Refund remaining balance to organizer\r\n    organizer.transfer(this.balance);\r\n    // Terminate contract\r\n    contract_terminated = true;\r\n  }\r\n\r\n  function finalize_contract() public {\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Make sure it's not the initialization stage anymore\r\n    assert(init_level == 3);\r\n    // Make sure the contract is finalized after the evaluation stage\r\n    assert(block.number >= init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size + evaluation_stage_block_size);\r\n    // Get the best submission to compare it against the criteria\r\n    Submission memory best_submission = submission_queue[best_submission_index];\r\n    // If best submission passes criteria, payout to the submitter\r\n    if (best_submission_accuracy >= model_accuracy_criteria) {\r\n      best_submission.payment_address.transfer(this.balance);\r\n    // If the best submission fails the criteria, refund the balance back to the organizer\r\n    } else {\r\n      organizer.transfer(this.balance);\r\n    }\r\n    contract_terminated = true;\r\n  }\r\n\r\n  function model_accuracy(uint submission_index, int256[datapoint_size][] data) public constant returns (int256){\r\n    // Make sure contract is not terminated\r\n    assert(contract_terminated == false);\r\n    // Make sure it's not the initialization stage anymore\r\n    assert(init_level == 3);\r\n    // Leave function public for offline error calculation\r\n    // Get's the sum error for the model\r\n    Submission memory sub = submission_queue[submission_index];\r\n    int256 true_prediction = 0;\r\n    int256 false_prediction = 0;\r\n    bool one_hot; // one-hot encoding if prediction size is 1 but model output size is 2\r\n    int[] memory prediction;\r\n    int[] memory ground_truth;\r\n    if ((prediction_size + 1) == sub.num_neurons_output_layer) {\r\n      one_hot = true;\r\n      prediction = new int[](sub.num_neurons_output_layer);\r\n      ground_truth = new int[](sub.num_neurons_output_layer);\r\n    } else {\r\n      one_hot = false;\r\n      prediction = new int[](prediction_size);\r\n      ground_truth = new int[](prediction_size);\r\n    }\r\n    for (uint i = 0; i < data.length; i++) {\r\n      // Get ground truth\r\n      for (uint j = datapoint_size-prediction_size; j < data[i].length; j++) {\r\n        uint d_index = j - datapoint_size + prediction_size;\r\n        // Only get prediction values\r\n        if (one_hot == true) {\r\n          if (data[i][j] == 0) {\r\n            ground_truth[d_index] = 1;\r\n            ground_truth[d_index + 1] = 0;\r\n          } else if (data[i][j] == 1) {\r\n            ground_truth[d_index] = 0;\r\n            ground_truth[d_index + 1] = 1;\r\n          } else {\r\n            // One-hot encoding for more than 2 classes is not supported\r\n            require(false);\r\n          }\r\n        } else {\r\n          ground_truth[d_index] = data[i][j];\r\n        }\r\n      }\r\n      // Get prediction\r\n      prediction = get_prediction(sub, data[i]);\r\n      // Get error for the output layer\r\n      for (uint k = 0; k < ground_truth.length; k++) {\r\n        if (ground_truth[k] == prediction[k]) {\r\n          true_prediction += 1;\r\n        } else {\r\n          false_prediction += 1;\r\n        }\r\n      }\r\n    }\r\n    // We multipl by int_precision to get up to x decimal point precision while\r\n    // calculating the accuracy\r\n    return (true_prediction * int_precision) / (true_prediction + false_prediction);\r\n  }\r\n\r\n  function get_train_data_length() public view returns(uint256) {\r\n    return train_data.length;\r\n  }\r\n\r\n  function get_test_data_length() public view returns(uint256) {\r\n    return test_data.length;\r\n  }\r\n\r\n  function round_up_division(int256 dividend, int256 divisor) private pure returns(int256) {\r\n    // A special trick since solidity normall rounds it down\r\n    return (dividend + divisor -1) / divisor;\r\n  }\r\n\r\n  function not_in_train_partition(uint[training_data_group_size/partition_size] partition, uint number) private pure returns (bool) {\r\n    for (uint i = 0; i < partition.length; i++) {\r\n      if (number == partition[i]) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n\r\n  function randomly_select_index(uint[] array) private {\r\n    uint t_index = 0;\r\n    uint array_length = array.length;\r\n    uint block_i = 0;\r\n    // Randomly select training indexes\r\n    while(t_index < training_partition.length) {\r\n      uint random_index = uint(sha256(block.blockhash(block.number-block_i))) % array_length;\r\n      training_partition[t_index] = array[random_index];\r\n      array[random_index] = array[array_length-1];\r\n      array_length--;\r\n      block_i++;\r\n      t_index++;\r\n    }\r\n    t_index = 0;\r\n    while(t_index < testing_partition.length) {\r\n      testing_partition[t_index] = array[array_length-1];\r\n      array_length--;\r\n      t_index++;\r\n    }\r\n  }\r\n\r\n  function valid_weights(int[] weights, uint num_neurons_input_layer, uint num_neurons_output_layer, uint[] num_neurons_hidden_layer) private pure returns (bool) {\r\n    // make sure the number of weights match the network structure\r\n    // get number of weights based on network structure\r\n    uint ns_total = 0;\r\n    uint wa_total = 0;\r\n    uint number_of_layers = 2 + num_neurons_hidden_layer.length;\r\n\r\n    if (number_of_layers == 2) {\r\n      ns_total = num_neurons_input_layer * num_neurons_output_layer;\r\n    } else {\r\n      for(uint i = 0; i < num_neurons_hidden_layer.length; i++) {\r\n        // Get weights between first hidden layer and input layer\r\n        if (i==0){\r\n          ns_total += num_neurons_input_layer * num_neurons_hidden_layer[i];\r\n        // Get weights between hidden layers\r\n        } else {\r\n          ns_total += num_neurons_hidden_layer[i-1] * num_neurons_hidden_layer[i];\r\n        }\r\n      }\r\n      // Get weights between last hidden layer and output layer\r\n      ns_total += num_neurons_hidden_layer[num_neurons_hidden_layer.length-1] * num_neurons_output_layer;\r\n    }\r\n    // get number of weights in the weights array\r\n    wa_total = weights.length;\r\n\r\n    return ns_total == wa_total;\r\n  }\r\n\r\n    function unpack_data_groups(int256[] _data_groups, bool is_train_data) private {\r\n    int256[datapoint_size][] memory merged_data_group = new int256[datapoint_size][](_data_groups.length/datapoint_size);\r\n\r\n    for (uint i = 0; i < _data_groups.length/datapoint_size; i++) {\r\n      for (uint j = 0; j < datapoint_size; j++) {\r\n        merged_data_group[i][j] = _data_groups[i*datapoint_size + j];\r\n      }\r\n    }\r\n    if (is_train_data == true) {\r\n      // Assign training data\r\n      for (uint k = 0; k < merged_data_group.length; k++) {\r\n        train_data.push(merged_data_group[k]);\r\n      }\r\n    } else {\r\n      // Assign testing data\r\n      for (uint l = 0; l < merged_data_group.length; l++) {\r\n        test_data.push(merged_data_group[l]);\r\n      }\r\n    }\r\n  }\r\n\r\n    function sha_data_group(int256[] data_group, int256 data_group_nonce) private pure returns (bytes32) {\r\n      // Extract the relevant data points for the given data group index\r\n      // We concat all data groups and add the nounce to the end of the array\r\n      // and get the sha256 for the array\r\n      uint index_tracker = 0;\r\n      uint256 total_size = datapoint_size * partition_size;\r\n      /* uint256 start_index = data_group_index * total_size;\r\n      uint256 iter_limit = start_index + total_size; */\r\n      int256[] memory all_data_points = new int256[](total_size+1);\r\n\r\n      for (uint256 i = 0; i < total_size; i++) {\r\n        all_data_points[index_tracker] = data_group[i];\r\n        index_tracker += 1;\r\n      }\r\n      // Add nonce to the whole array\r\n      all_data_points[index_tracker] = data_group_nonce;\r\n      // Return sha256 on all data points + nonce\r\n      return sha256(all_data_points);\r\n    }\r\n\r\n  function relu_activation(int256 x) private pure returns (int256) {\r\n    if (x < 0) {\r\n      return 0;\r\n    } else {\r\n      return x;\r\n    }\r\n  }\r\n\r\n  function get_layer(uint nn) private pure returns (int256[]) {\r\n    int256[] memory input_layer = new int256[](nn);\r\n    return input_layer;\r\n  }\r\n\r\n  function get_hidden_layers(uint[] l_nn) private pure returns (int256[]) {\r\n    uint total_nn = 0;\r\n    // Skip first and last layer since they're not hidden layers\r\n    for (uint i = 1; i < l_nn.length-1; i++) {\r\n      total_nn += l_nn[i];\r\n    }\r\n    int256[] memory hidden_layers = new int256[](total_nn);\r\n    return hidden_layers;\r\n  }\r\n\r\n  function access_hidden_layer(int256[] hls, uint[] l_nn, uint index) private pure returns (int256[]) {\r\n    // TODO: Bug is here, doesn't work for between last hidden and output layer\r\n    // Returns the hidden layer from the hidden layers array\r\n    int256[] memory hidden_layer = new int256[](l_nn[index+1]);\r\n    uint hidden_layer_index = 0;\r\n    uint start = 0;\r\n    uint end = 0;\r\n    for (uint i = 0; i < index; i++) {\r\n      start += l_nn[i+1];\r\n    }\r\n    for (uint j = 0; j < (index + 1); j++) {\r\n      end += l_nn[j+1];\r\n    }\r\n    for (uint h_i = start; h_i < end; h_i++) {\r\n      hidden_layer[hidden_layer_index] = hls[h_i];\r\n      hidden_layer_index += 1;\r\n    }\r\n    return hidden_layer;\r\n  }\r\n\r\n  function get_prediction(Submission sub, int[datapoint_size] data_point) private pure returns(int256[]) {\r\n    uint[] memory l_nn = new uint[](sub.num_neurons_hidden_layer.length + 2);\r\n    l_nn[0] = sub.num_neurons_input_layer;\r\n    for (uint i = 0; i < sub.num_neurons_hidden_layer.length; i++) {\r\n      l_nn[i+1] = sub.num_neurons_hidden_layer[i];\r\n    }\r\n    l_nn[sub.num_neurons_hidden_layer.length+1] = sub.num_neurons_output_layer;\r\n    return forward_pass(data_point, sub.weights, sub.biases, l_nn);\r\n  }\r\n\r\n  function forward_pass(int[datapoint_size] data_point, int256[] weights, int256[] biases, uint[] l_nn) private pure returns (int256[]) {\r\n    // Initialize neuron arrays\r\n    int256[] memory input_layer = get_layer(l_nn[0]);\r\n    int256[] memory hidden_layers = get_hidden_layers(l_nn);\r\n    int256[] memory output_layer = get_layer(l_nn[l_nn.length-1]);\r\n\r\n    // load inputs from input layer\r\n    for (uint input_i = 0; input_i < l_nn[0]; input_i++) {\r\n      input_layer[input_i] = data_point[input_i];\r\n    }\r\n    return forward_pass2(l_nn, input_layer, hidden_layers, output_layer, weights, biases);\r\n  }\r\n\r\n  function forward_pass2(uint[] l_nn, int256[] input_layer, int256[] hidden_layers, int256[] output_layer, int256[] weights, int256[] biases) public pure returns (int256[]) {\r\n    // index_counter[0] is weight index\r\n    // index_counter[1] is hidden_layer_index\r\n    uint[] memory index_counter = new uint[](2);\r\n    for (uint layer_i = 0; layer_i < (l_nn.length-1); layer_i++) {\r\n      int256[] memory current_layer;\r\n      int256[] memory prev_layer;\r\n      // If between input and first hidden layer\r\n      if (hidden_layers.length != 0) {\r\n        if (layer_i == 0) {\r\n          current_layer = access_hidden_layer(hidden_layers, l_nn, layer_i);\r\n          prev_layer = input_layer;\r\n        // If between output and last hidden layer\r\n        } else if (layer_i == (l_nn.length-2)) {\r\n          current_layer = output_layer;\r\n          prev_layer = access_hidden_layer(hidden_layers, l_nn, (layer_i-1));\r\n        // If between hidden layers\r\n        } else {\r\n          current_layer = access_hidden_layer(hidden_layers, l_nn, layer_i);\r\n          prev_layer = access_hidden_layer(hidden_layers, l_nn, layer_i-1);\r\n        }\r\n      } else {\r\n        current_layer = output_layer;\r\n        prev_layer = input_layer;\r\n      }\r\n      for (uint layer_neuron_i = 0; layer_neuron_i < current_layer.length; layer_neuron_i++) {\r\n        int total = 0;\r\n        for (uint prev_layer_neuron_i = 0; prev_layer_neuron_i < prev_layer.length; prev_layer_neuron_i++) {\r\n          total += prev_layer[prev_layer_neuron_i] * weights[index_counter[0]];\r\n          index_counter[0]++;\r\n        }\r\n        total += biases[layer_i];\r\n        total = total / int_precision; // Divide by int_precision to scale down\r\n        // If between output and last hidden layer\r\n        if (layer_i == (l_nn.length-2)) {\r\n            output_layer[layer_neuron_i] = relu_activation(total);\r\n        } else {\r\n            hidden_layers[index_counter[1]] = relu_activation(total);\r\n        }\r\n        index_counter[1]++;\r\n      }\r\n    }\r\n    return output_layer;\r\n  }\r\n\r\n  // Fallback function for sending ether to this contract\r\n  function () public payable {}\r\n}","ABI":"[{\"constant\":true,\"inputs\":[],\"name\":\"init1_block_height\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[],\"name\":\"init2\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[{\"name\":\"submission_index\",\"type\":\"uint256\"}],\"name\":\"evaluate_model\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"submission_index\",\"type\":\"uint256\"},{\"name\":\"data\",\"type\":\"int256[3][]\"}],\"name\":\"model_accuracy\",\"outputs\":[{\"name\":\"\",\"type\":\"int256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get_training_index\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256[16]\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"evaluation_stage_block_size\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"\",\"type\":\"uint256\"},{\"name\":\"\",\"type\":\"uint256\"}],\"name\":\"test_data\",\"outputs\":[{\"name\":\"\",\"type\":\"int256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get_testing_index\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256[4]\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[{\"name\":\"_test_data_groups\",\"type\":\"int256[]\"},{\"name\":\"_test_data_group_nonces\",\"type\":\"int256\"}],\"name\":\"reveal_test_data\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"paymentAddress\",\"type\":\"address\"},{\"name\":\"num_neurons_input_layer\",\"type\":\"uint256\"},{\"name\":\"num_neurons_output_layer\",\"type\":\"uint256\"},{\"name\":\"num_neurons_hidden_layer\",\"type\":\"uint256[]\"},{\"name\":\"weights\",\"type\":\"int256[]\"},{\"name\":\"biases\",\"type\":\"int256[]\"}],\"name\":\"get_submission_id\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"best_submission_index\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"use_test_data\",\"outputs\":[{\"name\":\"\",\"type\":\"bool\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[{\"name\":\"_train_data_groups\",\"type\":\"int256[]\"},{\"name\":\"_train_data_group_nonces\",\"type\":\"int256\"}],\"name\":\"init3\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"l_nn\",\"type\":\"uint256[]\"},{\"name\":\"input_layer\",\"type\":\"int256[]\"},{\"name\":\"hidden_layers\",\"type\":\"int256[]\"},{\"name\":\"output_layer\",\"type\":\"int256[]\"},{\"name\":\"weights\",\"type\":\"int256[]\"},{\"name\":\"biases\",\"type\":\"int256[]\"}],\"name\":\"forward_pass2\",\"outputs\":[{\"name\":\"\",\"type\":\"int256[]\"}],\"payable\":false,\"stateMutability\":\"pure\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[{\"name\":\"_hashed_data_groups\",\"type\":\"bytes32[20]\"},{\"name\":\"accuracy_criteria\",\"type\":\"int256\"},{\"name\":\"organizer_refund_address\",\"type\":\"address\"}],\"name\":\"init1\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"organizer\",\"outputs\":[{\"name\":\"\",\"type\":\"address\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"init_level\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"name\":\"testing_partition\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get_train_data_length\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"best_submission_accuracy\",\"outputs\":[{\"name\":\"\",\"type\":\"int256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[],\"name\":\"finalize_contract\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"contract_terminated\",\"outputs\":[{\"name\":\"\",\"type\":\"bool\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"init3_block_height\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get_submission_queue_length\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[{\"name\":\"payment_address\",\"type\":\"address\"},{\"name\":\"num_neurons_input_layer\",\"type\":\"uint256\"},{\"name\":\"num_neurons_output_layer\",\"type\":\"uint256\"},{\"name\":\"num_neurons_hidden_layer\",\"type\":\"uint256[]\"},{\"name\":\"weights\",\"type\":\"int256[]\"},{\"name\":\"biases\",\"type\":\"int256[]\"}],\"name\":\"submit_model\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"name\":\"training_partition\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"reveal_test_data_groups_block_size\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":false,\"inputs\":[],\"name\":\"cancel_contract\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"model_accuracy_criteria\",\"outputs\":[{\"name\":\"\",\"type\":\"int256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[{\"name\":\"\",\"type\":\"uint256\"},{\"name\":\"\",\"type\":\"uint256\"}],\"name\":\"train_data\",\"outputs\":[{\"name\":\"\",\"type\":\"int256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get_test_data_length\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"submission_stage_block_size\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"},{\"inputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"constructor\"},{\"payable\":true,\"stateMutability\":\"payable\",\"type\":\"fallback\"}]","ContractName":"Danku_demo","CompilerVersion":"v0.4.20-nightly.2017.12.8+commit.226bfe5b","OptimizationUsed":"0","Runs":"200","ConstructorArguments":"","Library":"","SwarmSource":"bzzr://95f7601eee82fbca9dd4d614f32b2efeab39e868434f6a5245a8bbb0e785e8c3"}]}